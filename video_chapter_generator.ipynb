{
 "cells": [
  {
   "metadata": {
    "id": "936b8576d21f6578"
   },
   "cell_type": "markdown",
   "source": [
    "# Automatic video chaptering with LLMs and TF-IDF\n",
    " \n",
    "- Accompanying Medium article: [Automatic video chaptering with LLMs and TF-IDF]()\n",
    "- [Github repository](https://github.com/Yannael/automatic-video-chaptering)\n",
    "- [Gradio demo on HuggingFace](https://huggingface.co/spaces/Yannael/video-chaptering)\n"
   ],
   "id": "936b8576d21f6578"
  },
  {
   "metadata": {
    "id": "4b2ce387523e8767"
   },
   "cell_type": "markdown",
   "source": [
    "# Install and import libraries\n",
    "\n",
    "This notebook makes use of the following libraries:\n",
    "\n",
    "- `youtube-transcript-api`: used to directly download the video transcript from Youtube\n",
    "- `openai` and `groq`: used to interact with LLMs (Llama 3 8B or GPT-4o-mini)\n",
    "- `gradio`: used to create a simple web interface to interact with the model\n",
    "\n",
    "Optional libraries (for video downloading and speech-to-text Whisper model)\n",
    "\n",
    "- `yt-dlp`: used to download the audio of a Youtube video\n",
    "- `faster_whisper`: used to get transcript from audio\n"
   ],
   "id": "4b2ce387523e8767"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Install libraries with:",
   "id": "2184361d0bba7580"
  },
  {
   "metadata": {
    "id": "145a6d764975a065",
    "ExecuteTime": {
     "end_time": "2024-07-29T11:53:39.403232Z",
     "start_time": "2024-07-29T11:53:24.331135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "\n",
    "!pip install -q youtube-transcript-api\n",
    "!pip install -q openai\n",
    "!pip install -q groq\n",
    "!pip install -q gradio\n",
    "\n",
    "!pip install -q yt_dlp\n",
    "!pip install -q faster_whisper\n"
   ],
   "id": "145a6d764975a065",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "id": "d3b292af3e8a9ebc"
   },
   "cell_type": "markdown",
   "source": "Load libraries\n",
   "id": "d3b292af3e8a9ebc"
  },
  {
   "metadata": {
    "id": "e385bd173a3f98b5",
    "ExecuteTime": {
     "end_time": "2024-09-09T07:30:29.577391Z",
     "start_time": "2024-09-09T07:30:27.038072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import re \n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import yt_dlp\n",
    "from faster_whisper import WhisperModel\n",
    "import torch\n",
    "\n",
    "# Load your keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Or add your keys here\n",
    "# GROQ_API_KEY = \"xxx\"\n",
    "# OPENAI_API_KEY = \"xxx\""
   ],
   "id": "e385bd173a3f98b5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define Youtube video ID, folder to store video, chapters, and resulting blog post.\n",
   "id": "f242a429e47c7295"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T07:30:38.274020Z",
     "start_time": "2024-09-09T07:30:38.272103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Usage example\n",
    "video_id = 'ErnWZxJovaM' # MIT Introduction to Deep Learning | 6.S191 - Alexander Amini\n",
    "#video_id = 'Unzc731iCUY' # How to speak - Patrick Winston - https://www.youtube.com/watch?v=Unzc731iCUY\n",
    "#video_id = 'zduSFxRajkE' # Let's build the GPT Tokenizer - Andrej Karpathy\n",
    "\n",
    "DATA_DIR = f\"tmp/{video_id}\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n"
   ],
   "id": "b459f93788c9de66",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "190c9725641aa431"
   },
   "cell_type": "markdown",
   "source": [
    "## 1) Get the video/audio transcript\n",
    "\n"
   ],
   "id": "190c9725641aa431"
  },
  {
   "metadata": {
    "id": "f3abeccf47cdbca"
   },
   "cell_type": "markdown",
   "source": [
    "### With YouTubeTranscriptApi"
   ],
   "id": "f3abeccf47cdbca"
  },
  {
   "metadata": {
    "id": "aa251880011c435d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0446799a-c103-445e-9151-603083f8cb1c",
    "ExecuteTime": {
     "end_time": "2024-09-09T07:30:40.860149Z",
     "start_time": "2024-09-09T07:30:39.941279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[\"en\"])\n",
    "transcript = [{'start': s['start'], 'text': s['text']} for s in transcript]\n",
    "transcript[0:4]"
   ],
   "id": "aa251880011c435d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 1.17, 'text': '[Music]'},\n",
       " {'start': 10.28, 'text': 'good afternoon everyone and welcome to'},\n",
       " {'start': 12.88, 'text': 'MIT sus1 191 my name is Alexander amini'},\n",
       " {'start': 16.84, 'text': \"and I'll be one of your instructors for\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "cb1e8698e477d6da",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "459233d7-7e81-478b-876e-f7d3e1e23445",
    "ExecuteTime": {
     "end_time": "2024-09-09T07:30:41.780164Z",
     "start_time": "2024-09-09T07:30:41.777050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "len(transcript)"
   ],
   "id": "cb1e8698e477d6da",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1789"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "id": "b6866413017a00cf"
   },
   "cell_type": "markdown",
   "source": [
    "### With Whisper\n",
    "\n"
   ],
   "id": "b6866413017a00cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Download audio"
   ],
   "metadata": {
    "id": "Y5FHy9st98S0"
   },
   "id": "Y5FHy9st98S0"
  },
  {
   "cell_type": "code",
   "source": [
    "def download_audio(video_id, DOWNLOAD_DIR=\"temp_download\"):\n",
    "\n",
    "    os.makedirs(f\"{DOWNLOAD_DIR}\", exist_ok=True)\n",
    "    os.makedirs(f\"{DOWNLOAD_DIR}/{video_id}\", exist_ok=True)\n",
    "\n",
    "    audio_path = f\"{DOWNLOAD_DIR}/{video_id}/{video_id}_audio.mp4\"\n",
    "\n",
    "    # Define options for yt-dlp\n",
    "    ydl_opts = {\n",
    "        'format': f'bestaudio',  # Select the best quality format\n",
    "        'outtmpl': audio_path\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        video_url = 'https://www.youtube.com/watch?v=' + video_id\n",
    "        ydl.download([video_url])\n",
    "\n",
    "\n",
    "    return audio_path\n",
    "\n",
    "# About 5 seconds for a one hour video (65MB of audio)\n",
    "#%time \n",
    "path_to_audio=download_audio(video_id, DATA_DIR)"
   ],
   "metadata": {
    "id": "wQne_n-Z9_pL",
    "ExecuteTime": {
     "end_time": "2024-08-28T18:25:15.928120Z",
     "start_time": "2024-08-28T18:25:09.567409Z"
    }
   },
   "id": "wQne_n-Z9_pL",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=xv7oV1QD3sE\n",
      "[youtube] xv7oV1QD3sE: Downloading webpage\n",
      "[youtube] xv7oV1QD3sE: Downloading ios player API JSON\n",
      "[youtube] xv7oV1QD3sE: Downloading android player API JSON\n",
      "[youtube] xv7oV1QD3sE: Downloading player bcd1f224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] xv7oV1QD3sE: nsig extraction failed: You may experience throttling for some formats\n",
      "         n = kGAfTKXzUyorHLTMUm ; player = https://www.youtube.com/s/player/bcd1f224/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] xv7oV1QD3sE: nsig extraction failed: You may experience throttling for some formats\n",
      "         n = U0aww6VcNcAgDLX1WW ; player = https://www.youtube.com/s/player/bcd1f224/player_ias.vflset/en_US/base.js\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] xv7oV1QD3sE: Downloading m3u8 information\n",
      "[info] xv7oV1QD3sE: Downloading 1 format(s): 140\n",
      "[download] Destination: tmp/xv7oV1QD3sE/xv7oV1QD3sE/xv7oV1QD3sE_audio.mp4\n",
      "[download] 100% of   41.05MiB in 00:00:01 at 28.54MiB/s    \n",
      "[FixupM4a] Correcting container of \"tmp/xv7oV1QD3sE/xv7oV1QD3sE/xv7oV1QD3sE_audio.mp4\"\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Transcribe with Whisper"
   ],
   "metadata": {
    "id": "XdYFA0Sj9_0l"
   },
   "id": "XdYFA0Sj9_0l"
  },
  {
   "metadata": {
    "id": "cbbc9b50c4da22fd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307,
     "referenced_widgets": [
      "0a0ac81d2d3441e9b3a5df76a1bd2554",
      "625a356a3c03490e94fd7fd4c6735dd5",
      "9eae25c0de28460d8eda4c81f36b12c8",
      "7aaeb859fd084de5a9e0a6a8c457530d",
      "638b115d08bb4367a18d8cb83be3eaba",
      "b85add037e794592a3d10faafed14769",
      "e6593af0c76145cb95e70163d8b7aa79",
      "a037da54f3af40e6a8f2762d39c9223d",
      "e29b8ba4a5034817a48d2467424f922b",
      "3990513292034aafae131b8045a203e9",
      "c1da0bb480624ff0b39f7dd9fc5f9d0e",
      "575173787b5c4eb1b445d2dec7bae387",
      "1ec2b1f01be44544bf5a4cb59f4e7499",
      "17a44e75a6414564b01a9f053ebe8687",
      "11b80bf54a9e42bc99d936b63b464dba",
      "95a2cbd3dfe346d4a4d5337389a644f2",
      "9fe82db2540845f985804af8c3712c40",
      "12cc093257a5494fa42b75977e79d62c",
      "328692e5bc6d40178ecc999b374f7632",
      "593ff4c702d3449fb4f548bdddbb6977",
      "974273a5cbb64630b28f49f4622181f1",
      "f19d9c65f1cf4af893d71fb2189beb5d",
      "d9346d22a11e4a5dbfc8e265d7603c4d",
      "bdb99e3a45464eb4a52d00538f5db3b9",
      "7391cc19998e426288d31a3ad58b67b1",
      "7b8979ebd440484aa62c64a70a95b585",
      "ef51ff208ad54d5880fe097273503134",
      "87fa8610eb184d06a5fa2525e0221fbf",
      "7856dd4417c14c76afd1b8fb3aa2d151",
      "f6c93bc03f6d4573913b489a73694c54",
      "362358622d4047e8b1fb24a958315a02",
      "25747c9a3e3e47bc8d1067d65026fa22",
      "9e99c0ed5ccb4de9bc67c77d2e805a8a",
      "de6bb81359e74ab3b26d3ab321d7c789",
      "20b2418166fa4d0caa928ac2d390ad78",
      "2b5f0fd2b89c48ebb1b5487691f35411",
      "feca3fad25ac4c4d8c71a3189f0ef1f7",
      "69efc6afcfd444ae9ecb9ca5e70e0d5f",
      "2827314ccbdc49079e3fea36c335b588",
      "8329efacee6b4d0bb321b11d56a357c7",
      "92b5ded7cbf4467086af505723ad4e6d",
      "a19f4a06f3374e568acb51007656cea2",
      "6ad99df8f8b244b3861a49cd66a8de9c",
      "160bcbf3fc7e47aa83ac839dcef9e542",
      "24a4c950d74a41f29bf8b074dbc2997f",
      "ed5fa7711fac4e08b9f3882a269dd612",
      "ca7e477fbecd4d00b29db417663d5d54",
      "4bc292ec121c432f8ce0d71b70c37e30",
      "7d596ba8913842a1ba53a37d3b4d6728",
      "d5dbed46f7844ea6851b42af3766470f",
      "ce34ab0c3707422090e4f801fd927805",
      "e5e7b727f99e4df4a2cc890771230d2f",
      "bb4794ca6cf742b3880fd16cb26ad28d",
      "090f3982133a4bb38a41cae71300389a",
      "a552975238a944d8a50d1ddf9400c287"
     ]
    },
    "outputId": "852fc5e0-0e3f-43f2-df60-3b4514838aa8"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a0ac81d2d3441e9b3a5df76a1bd2554"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/2.39k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "575173787b5c4eb1b445d2dec7bae387"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9346d22a11e4a5dbfc8e265d7603c4d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de6bb81359e74ab3b26d3ab321d7c789"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocabulary.json:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24a4c950d74a41f29bf8b074dbc2997f"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": null,
   "source": [
    "whisper_model = WhisperModel(\"large-v3\",\n",
    "                              device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                              compute_type=\"float16\",\n",
    "                            )"
   ],
   "id": "cbbc9b50c4da22fd"
  },
  {
   "metadata": {
    "id": "d3ca9c90b82fa755"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def speech_to_text(whisper_model, audio_file, initial_prompt=\"Use punctuation, like this.\", language=\"en\", segments=None):\n",
    "\n",
    "        segments, transcript_info = whisper_model.transcribe(audio_file,  initial_prompt=initial_prompt, language=language)\n",
    "        segments = list(segments)\n",
    "        segments = [\n",
    "            {\n",
    "                \"start\": round(s.start,2),\n",
    "                \"duration\": round(s.end-s.start,2),\n",
    "                \"text\": s.text,\n",
    "            }\n",
    "            for s in segments\n",
    "        ]\n",
    "\n",
    "        return segments"
   ],
   "id": "d3ca9c90b82fa755"
  },
  {
   "metadata": {
    "id": "692121ab18b5f135",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "41f83829-5518-4ecf-b1fb-20691ab8d7fb"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 14min 11s, sys: 14.7 s, total: 14min 25s\n",
      "Wall time: 13min 48s\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "#14 minutes for a 1h10 video on T4\n",
    "%time transcript = speech_to_text(whisper_model, audio_path)\n"
   ],
   "id": "692121ab18b5f135"
  },
  {
   "cell_type": "code",
   "source": [
    "transcript[0:3]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKTasl06iMyK",
    "outputId": "6878fad4-150c-4852-83c4-ea6075b72b21",
    "ExecuteTime": {
     "end_time": "2024-09-09T07:30:49.924302Z",
     "start_time": "2024-09-09T07:30:49.920603Z"
    }
   },
   "id": "WKTasl06iMyK",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 1.17, 'text': '[Music]'},\n",
       " {'start': 10.28, 'text': 'good afternoon everyone and welcome to'},\n",
       " {'start': 12.88, 'text': 'MIT sus1 191 my name is Alexander amini'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "id": "2634bb12bad96f22",
    "ExecuteTime": {
     "end_time": "2024-09-09T07:31:11.976579Z",
     "start_time": "2024-09-09T07:31:11.966415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(f\"{DATA_DIR}/{video_id}_transcript.json\", \"w\") as f:\n",
    "        json.dump(transcript, f, indent=4)"
   ],
   "id": "2634bb12bad96f22",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "8b9e0979f4750865"
   },
   "cell_type": "markdown",
   "source": [
    "## 2) Structure transcript in paragraphs\n",
    "\n",
    "This stage improves the transcript's readability (using an LLM) by:\n",
    " \n",
    "- adding punctuation\n",
    "- removing verbal tics\n",
    "- and adding appropriate line breaks\n",
    "\n",
    "The addition of linebreaks allows to separate the transcript in paragraphs.\n"
   ],
   "id": "8b9e0979f4750865"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Concatenate text\n",
    "\n",
    "We first start by concatenating the text of the transcript in order to send it to the LLM in chunks.\n",
    "\n",
    "This removes the timestamp information, which will be added back later (with the help of TF-IDF, in stage 3 below)."
   ],
   "metadata": {
    "id": "G9jU4bzD-Pm_"
   },
   "id": "G9jU4bzD-Pm_"
  },
  {
   "cell_type": "code",
   "source": [
    "def get_transcript_as_text(transcript):\n",
    "    temp_list = [s['text'] for s in transcript]\n",
    "    transcript_as_text = ' '.join(temp_list)\n",
    "\n",
    "    return transcript_as_text\n",
    "\n",
    "transcript_as_text = get_transcript_as_text(transcript)\n",
    "\n",
    "print(\"Number of characters: \"+str(len(transcript_as_text))+\"\\n\")\n",
    "\n",
    "#print(\"First 1000 characters: \")\n",
    "transcript_as_text[0:1000]\n",
    "#transcript_as_text"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "5wDi7NYZ-Xnu",
    "outputId": "efabfcbf-3ce6-4226-c7f3-a15e235c3e07",
    "ExecuteTime": {
     "end_time": "2024-09-09T07:31:48.204678Z",
     "start_time": "2024-09-09T07:31:48.187389Z"
    }
   },
   "id": "5wDi7NYZ-Xnu",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 66632\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[Music] good afternoon everyone and welcome to MIT sus1 191 my name is Alexander amini and I'll be one of your instructors for the course this year along with Ava and together we're really excited to welcome you to this really incredible course this is a very fast-paced and very uh intense one week that we're about to go through together right so we're going to cover the foundations of a also very fast-paced moving field and a field that has been rapidly changing over the past eight years that we have taught this course at MIT now over the past decade in fact even before we started teaching this course Ai and deep learning has really been revolutionizing so many different advances and so many different areas of science meth mathematics physics and and so on and not that long ago we were having new types of we were having challenges and problems that we did not think were necessarily solvable in our lifetimes that AI is now actually solving uh Beyond human performance today and each yea\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Get LLM client\n",
    "\n",
    "Groq llama3-8b-8192 is faster and cheaper than OpenAI gpt-4o-mini, but is slightly less accurate.\n",
    "\n",
    "If using Groq, change chunk_size_format_transcript to 1500 for better results (otherwise part of the input may go missing)."
   ],
   "id": "3bfa66771a1b97fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T07:45:53.840316Z",
     "start_time": "2024-09-09T07:45:53.807164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Uncomment below to use Groq\n",
    "#llm_client_format_transcript = Groq(api_key=GROQ_API_KEY)\n",
    "#llm_model_format_transcript = 'llama3-8b-8192'\n",
    "#chunk_size_format_transcript = 1500\n",
    "\n",
    "# Comment below to use GPT-4o-mini\n",
    "llm_client_format_transcript = OpenAI(api_key=OPENAI_API_KEY)\n",
    "llm_model_format_transcript= \"gpt-4o-mini-2024-07-18\"\n",
    "chunk_size_format_transcript = 5000"
   ],
   "id": "5b92b24f5704513d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Price list for different LLMs (as of September 2024):",
   "id": "58f2552a23980150"
  },
  {
   "cell_type": "code",
   "source": [
    "price_token={'gpt-4o': {'input': 5/1000000, 'output': 15/1000000},\n",
    "             'gpt-4o-2024-08-06': {'input': 2.5/1000000, 'output': 10/1000000},\n",
    "             'gpt-4o-mini-2024-07-18': {'input': 0.15/1000000, 'output': 0.6/1000000},\n",
    "             'llama3-8b-8192' : {'input': 0.05 / 1000000, 'output': 0.08 / 1000000},\n",
    "             'llama3-70b-8192' : {'input': 0.59 / 1000000, 'output': 0.79 / 1000000},\n",
    "             'claude-3-5-sonnet-20240620': {'input': 3/1000000, 'output': 15/1000000},\n",
    "             'claude-3-haiku-20240307': {'input': 0.25/1000000, 'output': 1.25/1000000},\n",
    "             }"
   ],
   "metadata": {
    "id": "gBl4FhkDpyQi",
    "ExecuteTime": {
     "end_time": "2024-09-09T07:45:55.125138Z",
     "start_time": "2024-09-09T07:45:55.122874Z"
    }
   },
   "id": "gBl4FhkDpyQi",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Call LLM\n",
    "\n",
    "The call_llm function sends a prompt to the LLM and returns the response. It also calculates the price based on the number of tokens used."
   ],
   "id": "dbe320b7a247900c"
  },
  {
   "cell_type": "code",
   "source": [
    "def call_llm(client, model, system_prompt, prompt,\n",
    "             temperature=0, seed=42, response_format=None, max_tokens=4000):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model=model, \n",
    "        temperature=temperature,\n",
    "        seed=seed,\n",
    "        response_format=response_format,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    nb_input_tokens = response.usage.prompt_tokens\n",
    "    nb_output_tokens = response.usage.completion_tokens\n",
    "    price = nb_input_tokens * price_token[model]['input'] + nb_output_tokens * price_token[model]['output']\n",
    "\n",
    "    print(f\"input tokens: {nb_input_tokens}; output tokens: {nb_output_tokens}, price: {price}\")\n",
    "\n",
    "    response_content=response.choices[0].message.content\n",
    "\n",
    "    return response_content, nb_input_tokens, nb_output_tokens, price\n"
   ],
   "metadata": {
    "id": "noAh3ZhhgnDy",
    "ExecuteTime": {
     "end_time": "2024-09-09T07:45:55.909695Z",
     "start_time": "2024-09-09T07:45:55.907006Z"
    }
   },
   "id": "noAh3ZhhgnDy",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The system prompt for the transcript formatting task is as follows:",
   "id": "ef4cf614a09e615c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T07:45:58.778638Z",
     "start_time": "2024-09-09T07:45:58.776557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt_transcript_to_paragraphs = f\"\"\"\n",
    "\n",
    "You are a helpful assistant.\n",
    "\n",
    "Your task is to improve the user input's readability: add punctuation if needed and remove verbal tics, and structure the text in paragraphs separated with '\\n\\n'.\n",
    "\n",
    "Keep the wording as faithful as possible to the original text. \n",
    "\n",
    "Put your answer within <answer></answer> tags.\n",
    "\n",
    "\"\"\""
   ],
   "id": "9edf2961f55f1b6b",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Test the LLM on a chunk of the transcript."
   ],
   "id": "8d541a7ec69bcdde"
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "response_content, nb_input_tokens, nb_output_tokens, price = \\\n",
    "            call_llm(llm_client_format_transcript, llm_model_format_transcript,\n",
    "                     system_prompt_transcript_to_paragraphs, transcript_as_text[0:chunk_size_format_transcript],\n",
    "                     temperature=0, seed=42, response_format=None)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-nat4pnoWPW",
    "outputId": "0d7c44c6-9ba4-4fcb-a70c-7bc26f0dc527",
    "ExecuteTime": {
     "end_time": "2024-09-09T07:46:08.341361Z",
     "start_time": "2024-09-09T07:45:59.908074Z"
    }
   },
   "id": "g-nat4pnoWPW",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: 1007; output tokens: 905, price: 0.00069405\n",
      "CPU times: user 9.66 ms, sys: 12.4 ms, total: 22.1 ms\n",
      "Wall time: 8.43 s\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T07:46:08.346942Z",
     "start_time": "2024-09-09T07:46:08.344333Z"
    }
   },
   "cell_type": "code",
   "source": "print(response_content)\n",
   "id": "1b5a6cf314eef9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<answer>[Music] Good afternoon, everyone, and welcome to MIT's 6.S191. My name is Alexander Amini, and I'll be one of your instructors for the course this year, along with Ava. Together, we're really excited to welcome you to this incredible course.\n",
      "\n",
      "This is a very fast-paced and intense one week that we're about to go through together. We will cover the foundations of a rapidly evolving field that has been changing significantly over the past eight years since we began teaching this course at MIT. In fact, over the past decade, even before we started teaching this course, AI and deep learning have been revolutionizing many different areas of science, mathematics, physics, and more.\n",
      "\n",
      "Not long ago, we faced challenges and problems that we did not think were necessarily solvable in our lifetimes. However, AI is now solving these problems, often exceeding human performance. Each year that we teach this course, this lecture in particular is becoming harder to teach. For an introductory level course, this first lecture is supposed to cover the foundations. If you think about any other introductory course, like a 101 course in mathematics or biology, those first lectures don't change much over time. But we are in a rapidly changing field of AI and deep learning, where even these foundational lectures are evolving quickly.\n",
      "\n",
      "Let me give you an example of how we introduced this course only a few years ago. We welcomed everyone to MIT 6.S191, the official introductory course on deep learning taught here at MIT. Deep learning is revolutionizing many fields, from robotics to medicine and everything in between. In this course, you'll learn the fundamentals of this field and how to build incredible algorithms. In fact, the entire speech and video you just watched were not real; they were created using deep learning and artificial intelligence. In this class, you'll learn how it has been an honor to speak with you today, and I hope you enjoy the course.\n",
      "\n",
      "The really surprising thing about that video, when we first created it, was how viral it went a few years ago. Within just a couple of months of us teaching this course, that video received over a million views. People were shocked by a few things, but the main one was the realism of AI in generating content that looks and sounds extremely hyper-realistic. When we created this video for the class, it cost us about $10,000 in compute to generate just about a minute-long video. If you think about it, that's extremely expensive for computing something like that. \n",
      "\n",
      "Today, many of you might not even be impressed by the technology because you see all the amazing things that AI and deep learning are producing now. Fast forward to today, the progress in deep learning has been remarkable. People were making all kinds of exciting remarks about it when it first came out a few years ago, but now this is common technology. AI is now doing much more powerful things than that fun little introductory video.\n",
      "\n",
      "So, where are we now, about four years later? AI is generating content with deep learning being so commoditized. Deep learning is at our fingertips now, available online and on our smartphones. In fact, we can use deep learning to generate hyper-realistic pieces of media and content entirely from English language prompts, without even needing to code anymore. \n",
      "\n",
      "Before, we had to train these models and code them to create that one-minute-long video. Today, we have models that can do that for us end-to-end, directly from English language prompts. We can ask these models to create something that the world has never seen before, like a photo of an astronaut riding a horse. These models can imagine those pieces of content entirely from scratch.\n",
      "\n",
      "My personal favorite is how we can now ask these deep learning models to create new types of software. For example, we can ask them to write a piece of TensorFlow code to train a neural network. We are asking a neural network to write TensorFlow code to train another neural network, and our model can produce examples of functional and usable pieces of code that satisfy this English prompt, while also walking through each part of the code independently. \n",
      "\n",
      "This is not just about producing code; it's also about educating and teaching the user on what each part of these code blocks is doing. You can see examples of this, and what I'm trying to show you with all of this is how far deep learning has come, even in just a couple of years since we started teaching this course. \n",
      "\n",
      "Going back even further... </answer>\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Process the whole transcript\n",
    "\n",
    "Split the transcript in chunks and process iteratively."
   ],
   "id": "1c320e9c807f5728"
  },
  {
   "cell_type": "code",
   "source": [
    "def transcript_to_paragraphs(transcript, llm_client, llm_model, chunk_size=5000, progress=None):\n",
    "\n",
    "    transcript_as_text = ' '.join([s['text'] for s in transcript])\n",
    "\n",
    "    paragraphs = []\n",
    "    last_paragraph = \"\"\n",
    "\n",
    "    total_nb_input_tokens, total_nb_output_tokens, total_price = 0, 0, 0\n",
    "    \n",
    "    nb_chunks = int(len(transcript_as_text) / chunk_size) + 1\n",
    "    progress_i = 0\n",
    "    print(f\"Number of chunks: {nb_chunks}\")\n",
    "\n",
    "    #for i in range(0, 10000, chunk_size): \n",
    "    for i in range(0, len(transcript_as_text), chunk_size):\n",
    "        \n",
    "        print (\"i is: \"+str(i))\n",
    "        \n",
    "        chunk = last_paragraph + \" \" + transcript_as_text[i:i + chunk_size]\n",
    "        \n",
    "        if progress is not None:\n",
    "            progress_i += 1\n",
    "            progress(progress_i/nb_chunks, desc=\"Processing\")\n",
    "        \n",
    "        found_edited_transcript = False\n",
    "    \n",
    "        while not found_edited_transcript:\n",
    "\n",
    "            response_content, nb_input_tokens, nb_output_tokens, price = \\\n",
    "                call_llm(llm_client, llm_model, \n",
    "                     system_prompt = system_prompt_transcript_to_paragraphs, prompt = chunk,\n",
    "                     temperature=0.2, seed=42, response_format=None)\n",
    "        \n",
    "            # Sometimes the model 'forgets' to close the <answer> tag\n",
    "            if not \"</answer>\" in response_content:\n",
    "                response_content += \"</answer>\"\n",
    "                \n",
    "            # Extract content from <edited_transcript> tags\n",
    "            pattern = re.compile(r'<answer>(.*?)</answer>', re.DOTALL)\n",
    "            response_content_edited =  pattern.findall(response_content)\n",
    "            \n",
    "            if len(response_content_edited) > 0:\n",
    "                found_edited_transcript = True\n",
    "                response_content_edited = response_content_edited[0]\n",
    "            \n",
    "            else:\n",
    "                print(\"No edited transcript found. Trying again.\")\n",
    "                print(response_content[0:100])\n",
    "                print(response_content[-100:])\n",
    "                \n",
    "\n",
    "        total_nb_input_tokens += nb_input_tokens\n",
    "        total_nb_output_tokens += nb_output_tokens\n",
    "        total_price += price\n",
    "    \n",
    "        paragraphs_chunk = response_content_edited.strip().split('\\n\\n')\n",
    "\n",
    "        print('Found paragraphs:', len(paragraphs_chunk))\n",
    "        last_paragraph = paragraphs_chunk[-1]\n",
    "\n",
    "        paragraphs += paragraphs_chunk[:-1]\n",
    "\n",
    "    paragraphs += [last_paragraph]\n",
    "\n",
    "    paragraphs_dict = [{'paragraph_number': i, 'paragraph_text': paragraph} for i, paragraph in enumerate(paragraphs)]\n",
    "\n",
    "    return paragraphs_dict, total_nb_input_tokens, total_nb_output_tokens, total_price\n"
   ],
   "metadata": {
    "id": "xDKg5f2egBan",
    "ExecuteTime": {
     "end_time": "2024-09-09T07:33:15.307448Z",
     "start_time": "2024-09-09T07:33:15.300133Z"
    }
   },
   "id": "xDKg5f2egBan",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "id": "14c2033d59021116",
    "outputId": "8bec6958-73c3-4663-ac9a-adda8ee99690",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-09-09T07:34:11.533175Z",
     "start_time": "2024-09-09T07:33:37.695172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "paragraphs, nb_input_tokens, nb_output_tokens, price = transcript_to_paragraphs(transcript, llm_client_format_transcript, llm_model_format_transcript, chunk_size=chunk_size_format_transcript)"
   ],
   "id": "14c2033d59021116",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 45\n",
      "i is: 0\n",
      "input tokens: 359; output tokens: 284, price: 4.067e-05\n",
      "Found paragraphs: 4\n",
      "i is: 1500\n",
      "input tokens: 373; output tokens: 337, price: 4.5610000000000005e-05\n",
      "Found paragraphs: 2\n",
      "i is: 3000\n",
      "input tokens: 547; output tokens: 497, price: 6.711e-05\n",
      "Found paragraphs: 4\n",
      "i is: 4500\n",
      "input tokens: 490; output tokens: 423, price: 5.834e-05\n",
      "Found paragraphs: 4\n",
      "i is: 6000\n",
      "input tokens: 351; output tokens: 230, price: 3.5950000000000006e-05\n",
      "Found paragraphs: 2\n",
      "i is: 7500\n",
      "input tokens: 455; output tokens: 337, price: 4.971e-05\n",
      "Found paragraphs: 4\n",
      "i is: 9000\n",
      "input tokens: 414; output tokens: 350, price: 4.8700000000000005e-05\n",
      "Found paragraphs: 3\n",
      "i is: 10500\n",
      "input tokens: 553; output tokens: 499, price: 6.757e-05\n",
      "Found paragraphs: 4\n",
      "i is: 12000\n",
      "input tokens: 525; output tokens: 483, price: 6.489000000000001e-05\n",
      "Found paragraphs: 3\n",
      "i is: 13500\n",
      "input tokens: 460; output tokens: 404, price: 5.5320000000000006e-05\n",
      "Found paragraphs: 5\n",
      "i is: 15000\n",
      "input tokens: 371; output tokens: 313, price: 4.359e-05\n",
      "Found paragraphs: 3\n",
      "i is: 16500\n",
      "input tokens: 415; output tokens: 360, price: 4.9550000000000005e-05\n",
      "Found paragraphs: 6\n",
      "i is: 18000\n",
      "input tokens: 381; output tokens: 345, price: 4.665e-05\n",
      "Found paragraphs: 3\n",
      "i is: 19500\n",
      "input tokens: 463; output tokens: 434, price: 5.787e-05\n",
      "Found paragraphs: 5\n",
      "i is: 21000\n",
      "input tokens: 424; output tokens: 274, price: 4.312e-05\n",
      "Found paragraphs: 3\n",
      "i is: 22500\n",
      "input tokens: 387; output tokens: 375, price: 4.935000000000001e-05\n",
      "Found paragraphs: 1\n",
      "i is: 24000\n",
      "input tokens: 781; output tokens: 752, price: 9.921e-05\n",
      "Found paragraphs: 5\n",
      "i is: 25500\n",
      "input tokens: 619; output tokens: 356, price: 5.9430000000000005e-05\n",
      "Found paragraphs: 3\n",
      "i is: 27000\n",
      "input tokens: 440; output tokens: 420, price: 5.560000000000001e-05\n",
      "Found paragraphs: 3\n",
      "i is: 28500\n",
      "input tokens: 445; output tokens: 413, price: 5.529000000000001e-05\n",
      "Found paragraphs: 4\n",
      "i is: 30000\n",
      "input tokens: 365; output tokens: 333, price: 4.489e-05\n",
      "Found paragraphs: 3\n",
      "i is: 31500\n",
      "input tokens: 460; output tokens: 355, price: 5.14e-05\n",
      "Found paragraphs: 3\n",
      "i is: 33000\n",
      "input tokens: 439; output tokens: 400, price: 5.395e-05\n",
      "Found paragraphs: 5\n",
      "i is: 34500\n",
      "input tokens: 474; output tokens: 424, price: 5.762000000000001e-05\n",
      "Found paragraphs: 3\n",
      "i is: 36000\n",
      "input tokens: 573; output tokens: 471, price: 6.633e-05\n",
      "Found paragraphs: 5\n",
      "i is: 37500\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInternalServerError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:1\u001B[0m\n",
      "Cell \u001B[0;32mIn[15], line 30\u001B[0m, in \u001B[0;36mtranscript_to_paragraphs\u001B[0;34m(transcript, llm_client, llm_model, chunk_size, progress)\u001B[0m\n\u001B[1;32m     25\u001B[0m found_edited_transcript \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m found_edited_transcript:\n\u001B[1;32m     29\u001B[0m     response_content, nb_input_tokens, nb_output_tokens, price \u001B[38;5;241m=\u001B[39m \\\n\u001B[0;32m---> 30\u001B[0m         call_llm(llm_client, llm_model, \n\u001B[1;32m     31\u001B[0m              system_prompt \u001B[38;5;241m=\u001B[39m system_prompt_transcript_to_paragraphs, prompt \u001B[38;5;241m=\u001B[39m chunk,\n\u001B[1;32m     32\u001B[0m              temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, response_format\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;66;03m# Sometimes the model 'forgets' to close the <answer> tag\u001B[39;00m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m</answer>\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m response_content:\n",
      "Cell \u001B[0;32mIn[10], line 4\u001B[0m, in \u001B[0;36mcall_llm\u001B[0;34m(client, model, system_prompt, prompt, temperature, seed, response_format, max_tokens)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_llm\u001B[39m(client, model, system_prompt, prompt,\n\u001B[1;32m      2\u001B[0m              temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, response_format\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, max_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4000\u001B[39m):\n\u001B[0;32m----> 4\u001B[0m     response \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mchat\u001B[38;5;241m.\u001B[39mcompletions\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[1;32m      5\u001B[0m         messages\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m      6\u001B[0m             {\n\u001B[1;32m      7\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: system_prompt\n\u001B[1;32m      9\u001B[0m             },\n\u001B[1;32m     10\u001B[0m             {\n\u001B[1;32m     11\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     12\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt\n\u001B[1;32m     13\u001B[0m             }\n\u001B[1;32m     14\u001B[0m         ],\n\u001B[1;32m     15\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel, \n\u001B[1;32m     16\u001B[0m         temperature\u001B[38;5;241m=\u001B[39mtemperature,\n\u001B[1;32m     17\u001B[0m         seed\u001B[38;5;241m=\u001B[39mseed,\n\u001B[1;32m     18\u001B[0m         response_format\u001B[38;5;241m=\u001B[39mresponse_format,\n\u001B[1;32m     19\u001B[0m         max_tokens\u001B[38;5;241m=\u001B[39mmax_tokens\n\u001B[1;32m     20\u001B[0m     )\n\u001B[1;32m     22\u001B[0m     nb_input_tokens \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39musage\u001B[38;5;241m.\u001B[39mprompt_tokens\n\u001B[1;32m     23\u001B[0m     nb_output_tokens \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39musage\u001B[38;5;241m.\u001B[39mcompletion_tokens\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/groq/resources/chat/completions.py:289\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    177\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[1;32m    178\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m    179\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;124;03m    Creates a model response for the given chat conversation.\u001B[39;00m\n\u001B[1;32m    181\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001B[39;00m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/openai/v1/chat/completions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    291\u001B[0m         body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[1;32m    292\u001B[0m             {\n\u001B[1;32m    293\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n\u001B[1;32m    294\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[1;32m    295\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrequency_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: frequency_penalty,\n\u001B[1;32m    296\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction_call\u001B[39m\u001B[38;5;124m\"\u001B[39m: function_call,\n\u001B[1;32m    297\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunctions\u001B[39m\u001B[38;5;124m\"\u001B[39m: functions,\n\u001B[1;32m    298\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogit_bias\u001B[39m\u001B[38;5;124m\"\u001B[39m: logit_bias,\n\u001B[1;32m    299\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: logprobs,\n\u001B[1;32m    300\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[1;32m    301\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m: n,\n\u001B[1;32m    302\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparallel_tool_calls\u001B[39m\u001B[38;5;124m\"\u001B[39m: parallel_tool_calls,\n\u001B[1;32m    303\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpresence_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: presence_penalty,\n\u001B[1;32m    304\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: response_format,\n\u001B[1;32m    305\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m: seed,\n\u001B[1;32m    306\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop,\n\u001B[1;32m    307\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n\u001B[1;32m    308\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[1;32m    309\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n\u001B[1;32m    310\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[1;32m    311\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_logprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_logprobs,\n\u001B[1;32m    312\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[1;32m    313\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m: user,\n\u001B[1;32m    314\u001B[0m             },\n\u001B[1;32m    315\u001B[0m             completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParams,\n\u001B[1;32m    316\u001B[0m         ),\n\u001B[1;32m    317\u001B[0m         options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[1;32m    318\u001B[0m             extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[1;32m    319\u001B[0m         ),\n\u001B[1;32m    320\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mChatCompletion,\n\u001B[1;32m    321\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    322\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mStream[ChatCompletionChunk],\n\u001B[1;32m    323\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/groq/_base_client.py:1225\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1212\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1213\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1220\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1221\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1222\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1223\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1224\u001B[0m     )\n\u001B[0;32m-> 1225\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/groq/_base_client.py:920\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    912\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    913\u001B[0m     cast_to: Type[ResponseT],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    918\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    919\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m    921\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m    922\u001B[0m         options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m    923\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    924\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m    925\u001B[0m         remaining_retries\u001B[38;5;241m=\u001B[39mremaining_retries,\n\u001B[1;32m    926\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/groq/_base_client.py:1018\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1015\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1017\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1018\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[1;32m   1021\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   1022\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1025\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1026\u001B[0m )\n",
      "\u001B[0;31mInternalServerError\u001B[0m: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:51:47.330752Z",
     "start_time": "2024-09-08T11:51:47.310613Z"
    }
   },
   "cell_type": "code",
   "source": "len(paragraphs)",
   "id": "95624586f1a934a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Example of the first paragraphs",
   "id": "936699fe7d755bce"
  },
  {
   "metadata": {
    "id": "bf5640596a00415e",
    "outputId": "7cdcf124-f964-4868-f60b-3cb1ee16112d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-09-08T11:51:49.166802Z",
     "start_time": "2024-09-08T11:51:49.164120Z"
    }
   },
   "cell_type": "code",
   "source": "paragraphs[0:3]",
   "id": "bf5640596a00415e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paragraph_number': 0, 'paragraph_text': 'Hi everyone. '},\n",
       " {'paragraph_number': 1,\n",
       "  'paragraph_text': \"In this video, I'd like us to cover the process of tokenization in large language models. Now, you see here that I have a set face, and that's because tokenization is my least favorite part of working with large language models. Unfortunately, it is necessary to understand in some detail because it is fairly hairy and gnarly. There are a lot of hidden foot guns to be aware of, and much of the oddness with large language models typically traces back to tokenization. \"},\n",
       " {'paragraph_number': 2,\n",
       "  'paragraph_text': 'So, what is tokenization? In my previous video, \"Let\\'s Build GPT from Scratch,\" we actually already did tokenization, but we did a very naive, simple version of it. When you go to the Google Colab for that video, you will see that we loaded our training set, which was the Shakespeare dataset. In the beginning, the Shakespeare dataset is just a large string in Python; it\\'s just text. The question is, how do we plug text into large language models? '}]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the paragraphs in a JSON file",
   "id": "a3611c3db8b7cb6"
  },
  {
   "cell_type": "code",
   "source": [
    "with open(f\"{DATA_DIR}/{video_id}_paragraphs.json\", \"w\") as f:\n",
    "        json.dump(paragraphs, f, indent=4)"
   ],
   "metadata": {
    "id": "JDKP4rTDhzET",
    "ExecuteTime": {
     "end_time": "2024-09-08T11:51:53.664102Z",
     "start_time": "2024-09-08T11:51:53.659769Z"
    }
   },
   "id": "JDKP4rTDhzET",
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {
    "id": "e6f2e71c0a57eaa7"
   },
   "cell_type": "markdown",
   "source": [
    "## 3) Infer paragraph timestamps \n",
    "\n",
    "Let us now add back the timestamps to the paragraphs, using TF-IDF to match the paragraphs to the transcript segments.\n",
    "\n",
    "The transform_text_segments function takes a list of text segments and combines them into a list of segments, each containing a specified number of words. \n",
    "\n",
    "For example, given the five following text segments from the transcript:\n",
    "\n",
    "```\n",
    "transcript[0:5]\n",
    "[{'start': 1.17, 'text': '[Music]'},\n",
    " {'start': 10.28, 'text': 'good afternoon everyone and welcome to'},\n",
    " {'start': 12.88, 'text': 'MIT sus1 191 my name is Alexander amini'},\n",
    " {'start': 16.84, 'text': \"and I'll be one of your instructors for\"},\n",
    " {'start': 18.32, 'text': 'the course this year along with Ava and'}]\n",
    "```\n",
    "\n",
    "Call the transform_text_segments function with a num_words parameter of 10 would combine these segments into the following segments:\n",
    "\n",
    "```\n",
    "['[Music] good afternoon everyone and welcome to MIT sus1 191',\n",
    " 'good afternoon everyone and welcome to MIT sus1 191 my',\n",
    " \"MIT sus1 191 my name is Alexander amini and I'll\",\n",
    " \"and I'll be one of your instructors for the course\",\n",
    " 'the course this year along with Ava and']\n",
    "```\n",
    "\n",
    "Each segment contains a maximum of 10 words, and the last segment contains the remaining words from the original segments. This will be useful for the next step, where we will match the paragraphs to the transcript segments using TF-IDF."
   ],
   "id": "e6f2e71c0a57eaa7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:51:56.202285Z",
     "start_time": "2024-09-08T11:51:56.197887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transform_text_segments(text_segments, num_words=50):\n",
    "    # Initialize variables\n",
    "    transformed_segments = []\n",
    "    current_index = 0\n",
    "    num_segments = len(text_segments)\n",
    "\n",
    "    for i in range(num_segments):\n",
    "\n",
    "        current_index = i\n",
    "\n",
    "        # Get the current segment's starting timestamp and text\n",
    "        current_segment = text_segments[current_index]\n",
    "        current_text = current_segment['text']\n",
    "\n",
    "        # Initialize a list to hold the combined text\n",
    "        combined_text = \" \".join(current_text.split()[:num_words])\n",
    "        number_words_collected = len(current_text.split())\n",
    "\n",
    "        # Collect words from subsequent segments\n",
    "        while number_words_collected < num_words and (current_index + 1) < num_segments:\n",
    "            current_index += 1\n",
    "            next_segment = text_segments[current_index]\n",
    "            next_text = next_segment['text']\n",
    "            next_words = next_text.split()\n",
    "\n",
    "            # Append words from the next segment\n",
    "            if number_words_collected + len(next_words) <= num_words:\n",
    "                combined_text += ' ' + next_text\n",
    "                number_words_collected += len(next_words)\n",
    "            else:\n",
    "                # Only append enough words to reach the num_words limit\n",
    "                words_needed = num_words - number_words_collected\n",
    "                combined_text += ' ' + ' '.join(next_words[:words_needed])\n",
    "                number_words_collected = num_words\n",
    "\n",
    "        # Append the combined segment to the result\n",
    "        transformed_segments.append(combined_text)\n",
    "\n",
    "    return transformed_segments\n",
    "\n"
   ],
   "id": "e8d7831f7697231f",
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:51:56.951277Z",
     "start_time": "2024-09-08T11:51:56.948648Z"
    }
   },
   "cell_type": "code",
   "source": "transcript[0:5]",
   "id": "c9d95b0a991cd408",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0.04, 'text': \"hi everyone so in this video I'd like us\"},\n",
       " {'start': 2.04, 'text': 'to cover the process of tokenization in'},\n",
       " {'start': 4.08, 'text': 'large language models now you see here'},\n",
       " {'start': 6.44, 'text': \"that I have a set face and that's\"},\n",
       " {'start': 8.28, 'text': 'because uh tokenization is my least'}]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:51:57.648910Z",
     "start_time": "2024-09-08T11:51:57.646851Z"
    }
   },
   "cell_type": "code",
   "source": "transform_text_segments(transcript[0:5], num_words=10)",
   "id": "31f646d194080d21",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi everyone so in this video I'd like us to\",\n",
       " 'to cover the process of tokenization in large language models',\n",
       " 'large language models now you see here that I have',\n",
       " \"that I have a set face and that's because uh\",\n",
       " 'because uh tokenization is my least']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The add_timestamps_to_paragraphs function takes the transcript and the paragraphs and add back the timestamps to the paragraphs. It uses TF-IDF to find the most similar segment in the transcript for each paragraph.",
   "id": "b2da01e1fec34d2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:51:58.742086Z",
     "start_time": "2024-09-08T11:51:58.739131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_timestamps_to_paragraphs(transcript, paragraphs, num_words=50):\n",
    "    list_indices = []\n",
    "    \n",
    "    transcript_num_words = transform_text_segments(transcript, num_words=num_words)\n",
    "\n",
    "    paragraphs_start_text = [{\"start\": p['paragraph_number'], \"text\": p['paragraph_text']} for p in paragraphs]\n",
    "    paragraphs_num_words = transform_text_segments(paragraphs_start_text, num_words=num_words)\n",
    "    \n",
    "    # Create a TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer().fit_transform(transcript_num_words + paragraphs_num_words)\n",
    "    # Get the TF-IDF vectors for the transcript and the excerpt\n",
    "    vectors = vectorizer.toarray()\n",
    "    \n",
    "    for i in range(len(paragraphs_num_words)):\n",
    "        \n",
    "        # Extract the TF-IDF vector for the paragraph\n",
    "        paragraph_vector = vectors[len(transcript_num_words) + i]\n",
    "\n",
    "        # Calculate the cosine similarity between the paragraph vector and each transcript chunk\n",
    "        similarities = cosine_similarity(vectors[:len(transcript_num_words)], paragraph_vector.reshape(1, -1))\n",
    "        # Find the index of the most similar chunk\n",
    "        best_match_index = int(np.argmax(similarities))\n",
    "\n",
    "        list_indices.append(best_match_index)\n",
    "\n",
    "        paragraphs[i]['matched_index'] = best_match_index\n",
    "        paragraphs[i]['matched_text'] = transcript[best_match_index]['text']\n",
    "        paragraphs[i]['start_time'] = int(transcript[best_match_index]['start'])-2\n",
    "        if paragraphs[i]['start_time'] < 0:\n",
    "            paragraphs[i]['start_time'] = 0\n",
    "\n",
    "\n",
    "    return paragraphs"
   ],
   "id": "1868a850893fd82a",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:52:08.886624Z",
     "start_time": "2024-09-08T11:51:59.282126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "paragraphs = add_timestamps_to_paragraphs(transcript, paragraphs, num_words=50)"
   ],
   "id": "1a59469cc79aadb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.1 s, sys: 29.9 s, total: 1min 4s\n",
      "Wall time: 9.6 s\n"
     ]
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Example of the first paragraphs where the timestamps have been added back (start_time, in seconds):",
   "id": "c8669d86dce7e72c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:52:10.014805Z",
     "start_time": "2024-09-08T11:52:10.011960Z"
    }
   },
   "cell_type": "code",
   "source": "paragraphs[0:5]",
   "id": "4ded164aba383ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paragraph_number': 0,\n",
       "  'paragraph_text': 'Hi everyone. ',\n",
       "  'matched_index': 0,\n",
       "  'matched_text': \"hi everyone so in this video I'd like us\",\n",
       "  'start_time': 0},\n",
       " {'paragraph_number': 1,\n",
       "  'paragraph_text': \"In this video, I'd like us to cover the process of tokenization in large language models. Now, you see here that I have a set face, and that's because tokenization is my least favorite part of working with large language models. Unfortunately, it is necessary to understand in some detail because it is fairly hairy and gnarly. There are a lot of hidden foot guns to be aware of, and much of the oddness with large language models typically traces back to tokenization. \",\n",
       "  'matched_index': 1,\n",
       "  'matched_text': 'to cover the process of tokenization in',\n",
       "  'start_time': 0},\n",
       " {'paragraph_number': 2,\n",
       "  'paragraph_text': 'So, what is tokenization? In my previous video, \"Let\\'s Build GPT from Scratch,\" we actually already did tokenization, but we did a very naive, simple version of it. When you go to the Google Colab for that video, you will see that we loaded our training set, which was the Shakespeare dataset. In the beginning, the Shakespeare dataset is just a large string in Python; it\\'s just text. The question is, how do we plug text into large language models? ',\n",
       "  'matched_index': 13,\n",
       "  'matched_text': 'tokenization now in my previous video',\n",
       "  'start_time': 24},\n",
       " {'paragraph_number': 3,\n",
       "  'paragraph_text': 'In this case, we created a vocabulary of 65 possible characters that we saw occur in this string. These were the possible characters, and we saw that there are 65 of them. Then, we created a lookup table for converting every possible character, a little string piece, into a token, which is an integer. For example, we tokenized the string \"Hi\" and received a sequence of tokens. We took the first 1,000 characters of our dataset and encoded it into tokens. Because this is character-level tokenization, we received 1,000 tokens in a sequence, such as token 18, 47, etc. ',\n",
       "  'matched_index': 26,\n",
       "  'matched_text': 'here we created a vocabulary of 65',\n",
       "  'start_time': 56},\n",
       " {'paragraph_number': 4,\n",
       "  'paragraph_text': \"Later, we saw that the way we plug these tokens into the language model is by using an embedding table. If we have 65 possible tokens, then this embedding table is going to have 65 rows. Roughly speaking, we're taking the integer associated with every single token and using that as a lookup into this table. We pluck out the corresponding row, which contains trainable parameters that we're going to train using backpropagation. This is the vector that then feeds into the Transformer, and that's how the Transformer perceives every single token. \",\n",
       "  'matched_index': 43,\n",
       "  'matched_text': 'Etc now later we saw that the way we',\n",
       "  'start_time': 98}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the paragraphs with timestamps in a JSON file",
   "id": "4a23f5156d70dcb0"
  },
  {
   "metadata": {
    "id": "731c882780906b9b",
    "ExecuteTime": {
     "end_time": "2024-09-08T11:52:12.715706Z",
     "start_time": "2024-09-08T11:52:12.711353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(f\"{DATA_DIR}/{video_id}_paragraphs.json\", \"w\") as f:\n",
    "        json.dump(paragraphs, f, indent=4)"
   ],
   "id": "731c882780906b9b",
   "outputs": [],
   "execution_count": 155
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) Generate table of content\n",
    "\n",
    "The table of content is found by grouping consecutive paragraphs into chapters and identifying meaningful chapter titles.\n"
   ],
   "id": "a5f60c0c91f79a54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:52:13.458967Z",
     "start_time": "2024-09-08T11:52:13.454703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "paragraphs_number_text = [{'paragraph_number': p['paragraph_number'], 'paragraph_text': p['paragraph_text']} for p in paragraphs]\n",
    "paragraphs_json_dump = json.dumps(paragraphs_number_text)\n",
    "\n",
    "paragraphs_json_dump[0:1000]"
   ],
   "id": "4d9d0d40a4fcd8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"paragraph_number\": 0, \"paragraph_text\": \"Hi everyone. \"}, {\"paragraph_number\": 1, \"paragraph_text\": \"In this video, I\\'d like us to cover the process of tokenization in large language models. Now, you see here that I have a set face, and that\\'s because tokenization is my least favorite part of working with large language models. Unfortunately, it is necessary to understand in some detail because it is fairly hairy and gnarly. There are a lot of hidden foot guns to be aware of, and much of the oddness with large language models typically traces back to tokenization. \"}, {\"paragraph_number\": 2, \"paragraph_text\": \"So, what is tokenization? In my previous video, \\\\\"Let\\'s Build GPT from Scratch,\\\\\" we actually already did tokenization, but we did a very naive, simple version of it. When you go to the Google Colab for that video, you will see that we loaded our training set, which was the Shakespeare dataset. In the beginning, the Shakespeare dataset is just a large string in Python; it\\'s ju'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:52:13.933654Z",
     "start_time": "2024-09-08T11:52:13.931814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt_paragraphs_to_toc = \"\"\"\n",
    "\n",
    "\tYou are a helpful assistant.\n",
    "\n",
    "\tYou are given a transcript of a course in JSON format as a list of paragraphs, each containing 'paragraph_number' and 'paragraph_text' keys.\n",
    "\n",
    "\tYour task is to group consecutive paragraphs in chapters for the course and identify meaningful chapter titles.\n",
    "\n",
    "\tHere are the steps to follow:\n",
    "\n",
    "1. Read the transcript carefully to understand its general structure and the main topics covered.\n",
    "2. Look for clues that a new chapter is about to start. This could be a change of topic, a change of time or setting, the introduction of new themes or topics, or the speaker's explicit mention of a new part.\n",
    "3. For each chapter, keep track of the paragraph number that starts the chapter and identify a meaningful chapter title.\n",
    "4. Chapters should ideally be equally spaced throughout the transcript, and discuss a specific topic.\n",
    "\n",
    "\tFormat your result in JSON, with a list dictionaries for chapters, with 'start_paragraph_number':integer and 'title':string as key:value.\n",
    "\t\n",
    "\tExample: \n",
    "    {\"chapters\": \n",
    "        [{\"start_paragraph_number\": 0, \"title\": \"Introduction\"}, \n",
    "         {\"start_paragraph_number\": 10, \"title\": \"Chapter 1\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\"\"\""
   ],
   "id": "63acab68339a149e",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use GPT-4o-mini for this task, as it is more cost-effective than OpenAI's GPT-4o and generally provides good results for this specific task.",
   "id": "b9e7beefd3526588"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:52:15.196847Z",
     "start_time": "2024-09-08T11:52:15.175712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm_client_get_toc = OpenAI(api_key=OPENAI_API_KEY)\n",
    "llm_model_get_toc= \"gpt-4o-mini-2024-07-18\"\n",
    "#llm_model_get_toc= \"gpt-4o-2024-08-06\"\n",
    "chunk_size_toc = 30"
   ],
   "id": "702b0fde3885304c",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:52:17.156523Z",
     "start_time": "2024-09-08T11:52:15.641298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "paragraphs_number_text = [{'paragraph_number': p['paragraph_number'], 'paragraph_text': p['paragraph_text']} for p in paragraphs]\n",
    "chunk_json_dump = json.dumps(paragraphs_number_text[0:chunk_size_toc])\n",
    "\n",
    "response, _, _, _ = call_llm(llm_client_get_toc, llm_model_get_toc, \\\n",
    "                    system_prompt_paragraphs_to_toc, chunk_json_dump, \\\n",
    "                    temperature=0, seed=42, response_format={\"type\": \"json_object\"})\n"
   ],
   "id": "686fa50474649618",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: 3145; output tokens: 140, price: 0.00055575\n"
     ]
    }
   ],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:52:17.161631Z",
     "start_time": "2024-09-08T11:52:17.157973Z"
    }
   },
   "cell_type": "code",
   "source": "print(response)",
   "id": "376c3f9b23a93135",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"chapters\": [\n",
      "    {\n",
      "      \"start_paragraph_number\": 0,\n",
      "      \"title\": \"Introduction to Tokenization\"\n",
      "    },\n",
      "    {\n",
      "      \"start_paragraph_number\": 5,\n",
      "      \"title\": \"Byte Pair Encoding and Advanced Tokenization\"\n",
      "    },\n",
      "    {\n",
      "      \"start_paragraph_number\": 10,\n",
      "      \"title\": \"Challenges and Complexities of Tokenization\"\n",
      "    },\n",
      "    {\n",
      "      \"start_paragraph_number\": 21,\n",
      "      \"title\": \"Tokenization in Non-English Languages\"\n",
      "    },\n",
      "    {\n",
      "      \"start_paragraph_number\": 26,\n",
      "      \"title\": \"Tokenization Issues in Programming Languages\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We generate the TOC sequentially on chunks of paragraphs as it generally provides better results.",
   "id": "b3af76ab074cc21a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:52:19.221951Z",
     "start_time": "2024-09-08T11:52:19.217546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def paragraphs_to_toc(paragraphs, llm_client, llm_model, chunk_size=100):\n",
    "\n",
    "    chapters = []\n",
    "    number_last_chapter = 0\n",
    "\n",
    "    total_nb_input_tokens, total_nb_output_tokens, total_price = 0, 0, 0\n",
    "\n",
    "    while number_last_chapter < len(paragraphs):\n",
    "\n",
    "        print(number_last_chapter)\n",
    "\n",
    "        chunk = paragraphs[number_last_chapter:(number_last_chapter + chunk_size)]\n",
    "        chunk = [{'paragraph_number': p['paragraph_number'], 'paragraph_text': p['paragraph_text']} for p in chunk]\n",
    "\n",
    "        chunk_json_dump = json.dumps(chunk)\n",
    "\n",
    "        content, nb_input_tokens, nb_output_tokens, price = call_llm(\\\n",
    "                llm_client, llm_model, \\\n",
    "                system_prompt_paragraphs_to_toc, chunk_json_dump, \\\n",
    "                temperature=0, seed=42, response_format={\"type\": \"json_object\"})\n",
    "\n",
    "        total_nb_input_tokens += nb_input_tokens\n",
    "        total_nb_output_tokens += nb_output_tokens\n",
    "        \n",
    "        chapters_chunk = json.loads(content)['chapters']\n",
    "        \n",
    "        if number_last_chapter == chapters_chunk[-1]['start_paragraph_number']:\n",
    "            break\n",
    "\n",
    "        chapters += chapters_chunk[:-1]\n",
    "        \n",
    "        number_last_chapter = chapters_chunk[-1]['start_paragraph_number']\n",
    "        if number_last_chapter >= len(paragraphs)-5:\n",
    "            break\n",
    "        \n",
    "    total_price = (total_nb_input_tokens * price_token[llm_model]['input'] + \n",
    "                   total_nb_output_tokens * price_token[llm_model]['output'])\n",
    "    \n",
    "    chapters += [chapters_chunk[-1]]\n",
    "\n",
    "    return chapters, total_nb_input_tokens, total_nb_output_tokens, total_price"
   ],
   "id": "7baa1cadb3bbdad7",
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:52:38.393767Z",
     "start_time": "2024-09-08T11:52:20.030268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "table_of_content, total_nb_input_tokens, total_nb_output_tokens, total_price = \\\n",
    "    paragraphs_to_toc(paragraphs, llm_client_get_toc, llm_model_get_toc, chunk_size=chunk_size_toc)"
   ],
   "id": "193dd5276bbf6de6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "input tokens: 3145; output tokens: 140, price: 0.00055575\n",
      "26\n",
      "input tokens: 2650; output tokens: 96, price: 0.00045509999999999995\n",
      "46\n",
      "input tokens: 3024; output tokens: 84, price: 0.000504\n",
      "68\n",
      "input tokens: 2807; output tokens: 92, price: 0.00047624999999999995\n",
      "90\n",
      "input tokens: 2812; output tokens: 84, price: 0.0004722\n",
      "114\n",
      "input tokens: 2992; output tokens: 60, price: 0.00048479999999999997\n",
      "125\n",
      "input tokens: 2454; output tokens: 84, price: 0.0004185\n",
      "153\n",
      "input tokens: 2533; output tokens: 114, price: 0.00044835\n",
      "180\n",
      "input tokens: 3008; output tokens: 86, price: 0.0005028\n",
      "206\n",
      "input tokens: 3156; output tokens: 89, price: 0.0005268\n",
      "230\n",
      "input tokens: 3092; output tokens: 139, price: 0.0005472\n",
      "250\n",
      "input tokens: 2940; output tokens: 64, price: 0.0004794\n",
      "274\n",
      "input tokens: 3206; output tokens: 81, price: 0.0005295\n",
      "295\n",
      "input tokens: 3234; output tokens: 155, price: 0.0005781\n",
      "323\n",
      "input tokens: 2646; output tokens: 140, price: 0.0004809\n"
     ]
    }
   ],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:53:51.423270Z",
     "start_time": "2024-09-08T11:53:51.410864Z"
    }
   },
   "cell_type": "code",
   "source": "table_of_content",
   "id": "15649ecb2537f6ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start_paragraph_number': 0, 'title': 'Introduction to Tokenization'},\n",
       " {'start_paragraph_number': 5,\n",
       "  'title': 'Byte Pair Encoding and Advanced Tokenization'},\n",
       " {'start_paragraph_number': 10,\n",
       "  'title': 'Challenges and Complexities of Tokenization'},\n",
       " {'start_paragraph_number': 21,\n",
       "  'title': 'Tokenization in Non-English Languages'},\n",
       " {'start_paragraph_number': 26,\n",
       "  'title': 'Tokenization and Efficiency in Python'},\n",
       " {'start_paragraph_number': 37,\n",
       "  'title': 'Understanding Unicode and Code Points'},\n",
       " {'start_paragraph_number': 46, 'title': 'Understanding Unicode Encodings'},\n",
       " {'start_paragraph_number': 60, 'title': 'Introduction to Byte Pair Encoding'},\n",
       " {'start_paragraph_number': 68,\n",
       "  'title': 'Introduction to Tokenization and Encoding'},\n",
       " {'start_paragraph_number': 86, 'title': 'Iterative Merging of Byte Pairs'},\n",
       " {'start_paragraph_number': 90, 'title': 'Introduction to Byte Pair Encoding'},\n",
       " {'start_paragraph_number': 102, 'title': 'Training the Tokenizer'},\n",
       " {'start_paragraph_number': 114, 'title': 'Encoding and Decoding Overview'},\n",
       " {'start_paragraph_number': 125, 'title': 'Implementing Token Encoding'},\n",
       " {'start_paragraph_number': 146,\n",
       "  'title': 'Handling Special Cases in Encoding'},\n",
       " {'start_paragraph_number': 153,\n",
       "  'title': 'Introduction to Byte Pair Encoding'},\n",
       " {'start_paragraph_number': 157, 'title': 'Exploring GPT-2 Tokenization'},\n",
       " {'start_paragraph_number': 165, 'title': 'Regex Patterns in Tokenization'},\n",
       " {'start_paragraph_number': 180, 'title': 'Tokenization and Regex Patterns'},\n",
       " {'start_paragraph_number': 197, 'title': 'Introduction to TikToken Library'},\n",
       " {'start_paragraph_number': 206, 'title': 'Understanding the GPT-2 Encoder'},\n",
       " {'start_paragraph_number': 215, 'title': 'Special Tokens in Tokenization'},\n",
       " {'start_paragraph_number': 230, 'title': 'Building Your Own GPT-4 Tokenizer'},\n",
       " {'start_paragraph_number': 239, 'title': 'Introduction to SentencePiece'},\n",
       " {'start_paragraph_number': 240,\n",
       "  'title': 'Understanding SentencePiece Functionality'},\n",
       " {'start_paragraph_number': 245,\n",
       "  'title': 'Configuring SentencePiece for Tokenization'},\n",
       " {'start_paragraph_number': 250,\n",
       "  'title': 'Introduction to Normalization and SentencePiece'},\n",
       " {'start_paragraph_number': 274, 'title': 'Vocabulary Size Considerations'},\n",
       " {'start_paragraph_number': 291, 'title': 'Multimodal Transformers'},\n",
       " {'start_paragraph_number': 295,\n",
       "  'title': 'Understanding Tokenization and Its Impact on LLM Performance'},\n",
       " {'start_paragraph_number': 301,\n",
       "  'title': 'Challenges with Non-English Languages and Arithmetic'},\n",
       " {'start_paragraph_number': 307,\n",
       "  'title': 'Tokenization Issues in Programming and Special Tokens'},\n",
       " {'start_paragraph_number': 312,\n",
       "  'title': 'Trailing Whitespace and Its Effects on LLM Outputs'},\n",
       " {'start_paragraph_number': 323, 'title': 'Understanding Unstable Tokens'},\n",
       " {'start_paragraph_number': 327,\n",
       "  'title': 'The Solid Gold Magikarp Phenomenon'},\n",
       " {'start_paragraph_number': 339,\n",
       "  'title': 'Tokenization Efficiency and Formats'},\n",
       " {'start_paragraph_number': 342, 'title': 'Final Thoughts on Tokenization'},\n",
       " {'start_paragraph_number': 350,\n",
       "  'title': 'Introduction to GPT-2 Encoder Code'}]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:54:02.751258Z",
     "start_time": "2024-09-08T11:54:02.747478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(f\"{DATA_DIR}/{video_id}_toc.json\", \"w\") as f:\n",
    "        json.dump(table_of_content, f, indent=4)"
   ],
   "id": "3f986e192097041d",
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {
    "id": "d55415a36ca7ca84"
   },
   "cell_type": "markdown",
   "source": [
    "## 5) Output structured chapter\n",
    "\n",
    "This last stage combines the paragraphs and the table of content to create a structured JSON with chapters."
   ],
   "id": "d55415a36ca7ca84"
  },
  {
   "cell_type": "code",
   "source": [
    "def get_chapters(paragraphs, table_of_content):\n",
    "\n",
    "    chapters = []\n",
    "\n",
    "    for i in range(len(table_of_content)):\n",
    "\n",
    "\n",
    "        if i < len(table_of_content) - 1:\n",
    "\n",
    "            chapter = {'num_chapter': i,\n",
    "                       'title': table_of_content[i]['title'],\n",
    "                       'start_paragraph_number': table_of_content[i]['start_paragraph_number'],\n",
    "                       'end_paragraph_number': table_of_content[i + 1]['start_paragraph_number'],\n",
    "                       'start_time': paragraphs[table_of_content[i]['start_paragraph_number']]['start_time'],\n",
    "                       'end_time': paragraphs[table_of_content[i + 1]['start_paragraph_number']]['start_time'],\n",
    "                      }\n",
    "\n",
    "        else:\n",
    "            chapter = {'num_chapter': i,\n",
    "                       'title': table_of_content[i]['title'],\n",
    "                       'start_paragraph_number': table_of_content[i]['start_paragraph_number'],\n",
    "                       'end_paragraph_number': len(paragraphs),\n",
    "                       'start_time': paragraphs[table_of_content[i]['start_paragraph_number']]['start_time'],\n",
    "                       'end_time': paragraphs[-1]['start_time'],\n",
    "                      }\n",
    "\n",
    "        paragraphs_chapter = [paragraphs[j]['paragraph_text'] for j in\n",
    "                                range(chapter['start_paragraph_number'], chapter['end_paragraph_number'])]\n",
    "\n",
    "        paragraph_timestamps_chapter = [paragraphs[j]['start_time'] for j in\n",
    "                                          range(chapter['start_paragraph_number'], chapter['end_paragraph_number'])]\n",
    "\n",
    "        chapter['paragraphs'] = paragraphs_chapter\n",
    "        chapter['paragraph_timestamps'] = paragraph_timestamps_chapter\n",
    "\n",
    "        chapters.append(chapter)\n",
    "\n",
    "    return chapters"
   ],
   "metadata": {
    "id": "Es_UUTejvAhe",
    "ExecuteTime": {
     "end_time": "2024-09-08T11:54:07.270566Z",
     "start_time": "2024-09-08T11:54:07.262164Z"
    }
   },
   "id": "Es_UUTejvAhe",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {
    "id": "feec2cdf35f1dbf8",
    "ExecuteTime": {
     "end_time": "2024-09-08T11:54:11.533981Z",
     "start_time": "2024-09-08T11:54:11.531316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chapters = get_chapters(paragraphs, table_of_content)\n"
   ],
   "id": "feec2cdf35f1dbf8",
   "outputs": [],
   "execution_count": 166
  },
  {
   "metadata": {
    "id": "6181283b929db001",
    "ExecuteTime": {
     "end_time": "2024-09-08T11:54:11.987544Z",
     "start_time": "2024-09-08T11:54:11.983615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(f\"{DATA_DIR}/{video_id}.json\", \"w\") as f:\n",
    "        json.dump(chapters, f, indent=4)"
   ],
   "id": "6181283b929db001",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:54:13.998463Z",
     "start_time": "2024-09-08T11:54:13.996027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_seconds_to_hms(seconds):\n",
    "    # Calculate hours, minutes, and remaining seconds\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    remaining_seconds = seconds % 60\n",
    "\n",
    "    # Format the result as HH:MM:SS\n",
    "    return f\"{hours:02}:{minutes:02}:{remaining_seconds:02}\""
   ],
   "id": "64af9ffed386c69b",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:54:14.479049Z",
     "start_time": "2024-09-08T11:54:14.476941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chapter in chapters:\n",
    "    print(convert_seconds_to_hms(chapter['start_time'])+\" : \"+chapter['title'])"
   ],
   "id": "5df8c8b4959d4d6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00 : Introduction to Tokenization\n",
      "00:02:19 : Byte Pair Encoding and Advanced Tokenization\n",
      "00:04:16 : Challenges and Complexities of Tokenization\n",
      "00:09:30 : Tokenization in Non-English Languages\n",
      "00:11:21 : Tokenization and Efficiency in Python\n",
      "00:14:59 : Understanding Unicode and Code Points\n",
      "00:18:11 : Understanding Unicode Encodings\n",
      "00:23:40 : Introduction to Byte Pair Encoding\n",
      "00:26:46 : Introduction to Tokenization and Encoding\n",
      "00:34:09 : Iterative Merging of Byte Pairs\n",
      "00:34:56 : Introduction to Byte Pair Encoding\n",
      "00:39:21 : Training the Tokenizer\n",
      "00:42:37 : Encoding and Decoding Overview\n",
      "00:48:21 : Implementing Token Encoding\n",
      "00:55:09 : Handling Special Cases in Encoding\n",
      "00:56:50 : Introduction to Byte Pair Encoding\n",
      "00:57:35 : Exploring GPT-2 Tokenization\n",
      "00:59:15 : Regex Patterns in Tokenization\n",
      "01:04:30 : Tokenization and Regex Patterns\n",
      "01:11:36 : Introduction to TikToken Library\n",
      "01:14:58 : Understanding the GPT-2 Encoder\n",
      "01:18:24 : Special Tokens in Tokenization\n",
      "01:25:25 : Building Your Own GPT-4 Tokenizer\n",
      "01:28:41 : Introduction to SentencePiece\n",
      "01:28:53 : Understanding SentencePiece Functionality\n",
      "01:30:48 : Configuring SentencePiece for Tokenization\n",
      "01:33:05 : Introduction to Normalization and SentencePiece\n",
      "01:43:26 : Vocabulary Size Considerations\n",
      "01:49:57 : Multimodal Transformers\n",
      "01:51:40 : Understanding Tokenization and Its Impact on LLM Performance\n",
      "01:54:14 : Challenges with Non-English Languages and Arithmetic\n",
      "01:56:48 : Tokenization Issues in Programming and Special Tokens\n",
      "01:58:44 : Trailing Whitespace and Its Effects on LLM Outputs\n",
      "02:03:18 : Understanding Unstable Tokens\n",
      "02:04:52 : The Solid Gold Magikarp Phenomenon\n",
      "02:09:14 : Tokenization Efficiency and Formats\n",
      "02:10:16 : Final Thoughts on Tokenization\n",
      "02:13:03 : Introduction to GPT-2 Encoder Code\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chapters to Markdown\n",
    "\n",
    "Let us convert the JSON chapters to Markdown format."
   ],
   "metadata": {
    "id": "GJZkjySKyD_W"
   },
   "id": "GJZkjySKyD_W"
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "GXgFkHuSytoK",
    "ExecuteTime": {
     "end_time": "2024-09-08T10:11:44.965559Z",
     "start_time": "2024-09-08T10:11:44.961841Z"
    }
   },
   "id": "GXgFkHuSytoK",
   "outputs": [],
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "source": [
    "def chapters_to_markdown(chapters):\n",
    "\n",
    "    markdown = \"\"\n",
    "\n",
    "    for i in range(len(chapters)):\n",
    "\n",
    "        chapter = chapters[i]\n",
    "\n",
    "        markdown += f\"# {chapter['title']}\\n\\n\"\n",
    "\n",
    "        for j in range(len(chapter['paragraphs'])):\n",
    "\n",
    "            paragraph = chapter['paragraphs'][j]\n",
    "            start_time = chapter['paragraph_timestamps'][j]\n",
    "            from_to = convert_seconds_to_hms(int(start_time))\n",
    "\n",
    "            markdown += f\"{from_to} - {paragraph}\\n\\n\"\n",
    "\n",
    "    return markdown\n"
   ],
   "metadata": {
    "id": "C5qGstgAyEIw",
    "ExecuteTime": {
     "end_time": "2024-09-08T10:11:45.132931Z",
     "start_time": "2024-09-08T10:11:45.130435Z"
    }
   },
   "id": "C5qGstgAyEIw",
   "outputs": [],
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "source": [
    "markdown = chapters_to_markdown(chapters)"
   ],
   "metadata": {
    "id": "raJNb93MyoOe",
    "ExecuteTime": {
     "end_time": "2024-09-08T10:11:45.857735Z",
     "start_time": "2024-09-08T10:11:45.855503Z"
    }
   },
   "id": "raJNb93MyoOe",
   "outputs": [],
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ],
   "metadata": {
    "id": "GFta1WQByqQi",
    "ExecuteTime": {
     "end_time": "2024-09-08T10:11:46.263206Z",
     "start_time": "2024-09-08T10:11:46.261358Z"
    }
   },
   "id": "GFta1WQByqQi",
   "outputs": [],
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "source": "printmd(markdown[0:1000])",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nqoXjlZSyqSn",
    "outputId": "ef0b5144-9b3b-4bc7-f0b9-b71904ce241a",
    "ExecuteTime": {
     "end_time": "2024-09-08T10:11:48.020326Z",
     "start_time": "2024-09-08T10:11:48.016047Z"
    }
   },
   "id": "nqoXjlZSyqSn",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# Introduction to Deep Learning\n\n00:00:00 - [Music] Good afternoon, everyone, and welcome to MIT's 6.S191. My name is Alexander Amini, and I'll be one of your instructors for the course this year, along with Ava. Together, we're really excited to welcome you to this incredible course.\n\n00:00:27 - This is a very fast-paced and intense one-week experience that we're about to go through together. We will cover the foundations of a rapidly evolving field that has been changing significantly over the past eight years that we have taught this course at MIT. In fact, over the past decade, even before we started teaching this course, AI and deep learning have been revolutionizing many different areas of science, mathematics, physics, and more.\n\n00:01:05 - Not long ago, we faced challenges and problems that we did not think were necessarily solvable in our lifetimes. Yet, AI is now solving these problems, often surpassing human performance. Each year that we teach this course, this particular l"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradio app\n",
    "\n",
    "Let us bundle all the stages in a Gradio app"
   ],
   "metadata": {
    "id": "bqc5yYQIo0B3"
   },
   "id": "bqc5yYQIo0B3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T18:47:59.965958Z",
     "start_time": "2024-08-28T18:47:58.172353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import json\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "import utils\n",
    "\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utils)\n"
   ],
   "id": "54788ddac1cbf6f0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/yalb/Projects/Github/video-chaptering-github/utils.py'>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T18:47:59.968621Z",
     "start_time": "2024-08-28T18:47:59.966946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#llm_client_format_transcript = Groq(api_key=GROQ_API_KEY)\n",
    "#llm_model_format_transcript = 'llama3-8b-8192'\n",
    "#chunk_size_format_transcript = 2000\n",
    "\n",
    "#llm_client_get_toc = OpenAI(api_key=OPENAI_API_KEY)\n",
    "#llm_model_get_toc= \"gpt-4o-mini-2024-07-18\"\n",
    "#chunk_size_toc = 30"
   ],
   "id": "7a8a0afb5bde6bbd",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T18:48:03.447090Z",
     "start_time": "2024-08-28T18:48:03.444508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_llm_client_and_model(llm_model):\n",
    "    \n",
    "    if llm_model == \"llama3-8b\":\n",
    "        llm_client = Groq(api_key=GROQ_API_KEY)\n",
    "        llm_model = 'llama3-8b-8192'\n",
    "    \n",
    "    elif llm_model == \"gpt-4o-mini\":\n",
    "        llm_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        llm_model = 'gpt-4o-mini-2024-07-18'\n",
    "    \n",
    "    return llm_client, llm_model\n",
    "        "
   ],
   "id": "e9c999c750cd4af8",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T18:48:04.047203Z",
     "start_time": "2024-08-28T18:48:04.043403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradio_process_video(video_id, \n",
    "                         model_format_transcript, model_toc,\n",
    "                         chunk_size_format_transcript, chunk_size_toc, \n",
    "                         progress=gr.Progress()):\n",
    "    \n",
    "    if video_id in [\"ErnWZxJovaM\"]:\n",
    "        chapters = utils.load_json_chapters(video_id)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[\"en\"])\n",
    "    \n",
    "        chunk_size_format_transcript = int(chunk_size_format_transcript)\n",
    "    \n",
    "        llm_client_format_transcript, llm_model_format_transcript = \\\n",
    "            get_llm_client_and_model(model_format_transcript)\n",
    "    \n",
    "        paragraphs, nb_input_tokens, nb_output_tokens, price = \\\n",
    "            utils.transcript_to_paragraphs(transcript,\\\n",
    "                                 llm_client_format_transcript, llm_model_format_transcript,\\\n",
    "                                 chunk_size=chunk_size_format_transcript, progress=progress)\n",
    "\n",
    "        paragraphs = utils.add_timestamps_to_paragraphs(transcript, paragraphs, num_words=50)\n",
    "    \n",
    "        chunk_size_toc = int(chunk_size_toc)\n",
    "        \n",
    "        llm_client_get_toc, llm_model_get_toc = \\\n",
    "            get_llm_client_and_model(model_toc)\n",
    "        \n",
    "        json_toc, nb_input_tokens, nb_output_tokens, price = \\\n",
    "        utils.paragraphs_to_toc(paragraphs, \\\n",
    "                        llm_client_get_toc, llm_model_get_toc, \\\n",
    "                        chunk_size=chunk_size_toc)\n",
    "    \n",
    "        chapters = utils.get_chapters(paragraphs, json_toc)\n",
    "    \n",
    "    output_html = utils.get_result_as_html(chapters, video_id)\n",
    "    \n",
    "    return {output_processing: str(output_html),\n",
    "            gv_output: output_html}\n",
    "    "
   ],
   "id": "b3325f12ef6817c4",
   "outputs": [],
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "source": [
    "css = \"\"\"\n",
    ".content {\n",
    "    padding: 20px;\n",
    "    max-width: 800px;\n",
    "    margin: 0 auto;\n",
    "    background-color: #ffffff;\n",
    "    box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "    border-radius: 8px;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "example_video_id = \"ErnWZxJovaM\"\n",
    "example_chapters = utils.load_json_chapters(example_video_id)\n",
    "example_output_html = utils.get_result_as_html(example_chapters, example_video_id)\n",
    "\n",
    "with (gr.Blocks(css=css) as app):\n",
    "    \n",
    "    \n",
    "    gr.HTML(\"<div align='center'><h1 class='header'>Demo: Automatic video chaptering with LLMs and TF-IDF</h1></div>\")\n",
    "    gr.HTML(\"<div align='center'><h3 class='header'>From raw transcript to structured document - Check the <a href=''>Medium article</a> for more details</h3></div>\")\n",
    "    gr.HTML(\"<hr>\")\n",
    "    gr.Markdown(\"\"\"This demo relies on \n",
    "                - Groq's Llama 3 8B for transcript preprocessing\n",
    "                - OpenAI's GPT-4o-mini for chaptering. Note: Using GPT-4o-mini for transcript preprocessing will improve results, but takes longer (around 2/3 minutes for a one-hour video)\n",
    "                \n",
    "                The following YouTube video ID are already preprocessed (copy and paste ID in box below): \n",
    "                \n",
    "                - `ErnWZxJovaM`: [MIT course](https://www.youtube.com/watch?v=ErnWZxJovaM)\n",
    "                - `EuC1GWhQdKE`: [Anthropic](https://www.youtube.com/watch?v=EuC1GWhQdKE)\n",
    "                \n",
    "                \"\"\"\n",
    "                )\n",
    "        \n",
    "    gv_transcript = gr.State()\n",
    "    \n",
    "    video_id_input = gr.Textbox(label=\"Enter YouTube Video ID\", value = \"EuC1GWhQdKE\")\n",
    "    \n",
    "    with gr.Accordion(\"Set parameters\", open=False):\n",
    "            \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                model_format_transcript = gr.Dropdown([(\"LLama 3 8B (Groq)\",\"llama3-8b\"), (\"GPT-4o-mini (OpenAI)\", \"gpt-4o-mini\")], label=\"Transcript preprocessing\", value=\"llama3-8b\", interactive=True)\n",
    "                chunk_size_format_transcript = gr.Textbox(label=\"Preprocessing chunk size\", value = 2000)\n",
    "            with gr.Column(scale=1):\n",
    "                model_toc = gr.Dropdown([(\"LLama 3 8B (Groq)\",\"llama3-8b\"), (\"GPT-4o-mini (OpenAI)\", \"gpt-4o-mini\")], label=\"Chaptering\", value=\"gpt-4o-mini\", interactive=True)\n",
    "                chunk_size_toc = gr.Textbox(label=\"Chaptering chunk size\", value = 30)\n",
    "            with gr.Column(scale=1):\n",
    "                api_key_openai = gr.Textbox(label=\"OpenAI API Key\", value = \"xxx\")\n",
    "                api_key_groq = gr.Textbox(label=\"Groq API Key\", value = \"xxx\")\n",
    "    \n",
    "    processing_button = gr.Button(\"Process transcript\")\n",
    "    \n",
    "    gv_output = gr.State()\n",
    "    \n",
    "    gr.HTML(\"<hr>\")\n",
    "    \n",
    "    output_processing = gr.HTML(label = \"Output processing\", value=example_output_html)\n",
    "    \n",
    "    processing_button.click(gradio_process_video,\n",
    "                            inputs=[video_id_input, \n",
    "                                    model_format_transcript, model_toc,\n",
    "                                    chunk_size_format_transcript, chunk_size_toc],\n",
    "                            outputs=[output_processing, gv_output])\n",
    "    \n",
    "    #gr.HTML(result_as_html)\n",
    "    \n",
    "\n",
    "app.launch(debug=True, width= \"100%\")"
   ],
   "metadata": {
    "id": "P4GMC275XoRA",
    "ExecuteTime": {
     "end_time": "2024-08-28T18:56:18.481971Z",
     "start_time": "2024-08-28T18:48:04.566678Z"
    }
   },
   "id": "P4GMC275XoRA",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 21\n",
      "i is: 0\n",
      "input tokens: 483; output tokens: 487, price: 0.00036465\n",
      "Found paragraphs: 30\n",
      "i is: 2000\n",
      "input tokens: 455; output tokens: 424, price: 0.00032265\n",
      "Found paragraphs: 11\n",
      "i is: 4000\n",
      "input tokens: 471; output tokens: 471, price: 0.00035325\n",
      "Found paragraphs: 14\n",
      "i is: 6000\n",
      "input tokens: 513; output tokens: 509, price: 0.00038235\n",
      "Found paragraphs: 24\n",
      "i is: 8000\n",
      "input tokens: 464; output tokens: 397, price: 0.0003078\n",
      "Found paragraphs: 11\n",
      "i is: 10000\n",
      "input tokens: 505; output tokens: 473, price: 0.00035955000000000004\n",
      "Found paragraphs: 26\n",
      "i is: 12000\n",
      "input tokens: 489; output tokens: 425, price: 0.00032834999999999993\n",
      "Found paragraphs: 21\n",
      "i is: 14000\n",
      "input tokens: 508; output tokens: 448, price: 0.000345\n",
      "Found paragraphs: 10\n",
      "i is: 16000\n",
      "input tokens: 505; output tokens: 476, price: 0.00036135\n",
      "Found paragraphs: 24\n",
      "i is: 18000\n",
      "input tokens: 447; output tokens: 422, price: 0.00032025\n",
      "Found paragraphs: 20\n",
      "i is: 20000\n",
      "input tokens: 463; output tokens: 433, price: 0.00032924999999999995\n",
      "Found paragraphs: 14\n",
      "i is: 22000\n",
      "input tokens: 485; output tokens: 456, price: 0.00034635\n",
      "Found paragraphs: 20\n",
      "i is: 24000\n",
      "input tokens: 457; output tokens: 433, price: 0.00032835\n",
      "Found paragraphs: 15\n",
      "i is: 26000\n",
      "input tokens: 481; output tokens: 450, price: 0.00034215\n",
      "Found paragraphs: 11\n",
      "i is: 28000\n",
      "input tokens: 467; output tokens: 466, price: 0.00034964999999999996\n",
      "Found paragraphs: 26\n",
      "i is: 30000\n",
      "input tokens: 461; output tokens: 426, price: 0.00032474999999999995\n",
      "Found paragraphs: 16\n",
      "i is: 32000\n",
      "input tokens: 473; output tokens: 453, price: 0.00034275\n",
      "Found paragraphs: 24\n",
      "i is: 34000\n",
      "input tokens: 467; output tokens: 437, price: 0.00033224999999999997\n",
      "Found paragraphs: 25\n",
      "i is: 36000\n",
      "input tokens: 464; output tokens: 460, price: 0.0003456\n",
      "Found paragraphs: 26\n",
      "i is: 38000\n",
      "input tokens: 479; output tokens: 462, price: 0.00034904999999999995\n",
      "Found paragraphs: 26\n",
      "i is: 40000\n",
      "input tokens: 399; output tokens: 352, price: 0.00027105\n",
      "Found paragraphs: 12\n",
      "0\n",
      "input tokens: 1165; output tokens: 83, price: 0.00022454999999999998\n",
      "19\n",
      "input tokens: 1534; output tokens: 125, price: 0.0003051\n",
      "44\n",
      "input tokens: 1443; output tokens: 62, price: 0.00025365\n",
      "50\n",
      "input tokens: 1406; output tokens: 93, price: 0.0002667\n",
      "68\n",
      "input tokens: 1411; output tokens: 86, price: 0.00026325\n",
      "85\n",
      "input tokens: 1214; output tokens: 63, price: 0.00021989999999999998\n",
      "107\n",
      "input tokens: 1436; output tokens: 63, price: 0.0002532\n",
      "129\n",
      "input tokens: 1514; output tokens: 63, price: 0.0002649\n",
      "138\n",
      "input tokens: 1309; output tokens: 37, price: 0.00021855\n",
      "Number of chunks: 9\n",
      "i is: 0\n",
      "input tokens: 1065; output tokens: 1162, price: 0.0008569499999999999\n",
      "Found paragraphs: 61\n",
      "i is: 5000\n",
      "input tokens: 1104; output tokens: 1142, price: 0.0008508\n",
      "Found paragraphs: 60\n",
      "i is: 10000\n",
      "input tokens: 1094; output tokens: 1122, price: 0.0008373\n",
      "Found paragraphs: 57\n",
      "i is: 15000\n",
      "input tokens: 1045; output tokens: 1063, price: 0.00079455\n",
      "Found paragraphs: 31\n",
      "i is: 20000\n",
      "input tokens: 1033; output tokens: 1038, price: 0.00077775\n",
      "Found paragraphs: 36\n",
      "i is: 25000\n",
      "input tokens: 1049; output tokens: 1103, price: 0.0008191499999999999\n",
      "Found paragraphs: 56\n",
      "i is: 30000\n",
      "input tokens: 1033; output tokens: 961, price: 0.0007315499999999999\n",
      "Found paragraphs: 27\n",
      "i is: 35000\n",
      "input tokens: 1088; output tokens: 1113, price: 0.0008309999999999999\n",
      "Found paragraphs: 63\n",
      "i is: 40000\n",
      "input tokens: 399; output tokens: 352, price: 0.00027105\n",
      "Found paragraphs: 12\n",
      "0\n",
      "input tokens: 1832; output tokens: 118, price: 0.0003456\n",
      "45\n",
      "input tokens: 1966; output tokens: 65, price: 0.00033390000000000004\n",
      "68\n",
      "input tokens: 1881; output tokens: 86, price: 0.00033375\n",
      "104\n",
      "input tokens: 1908; output tokens: 64, price: 0.0003246\n",
      "142\n",
      "input tokens: 2336; output tokens: 60, price: 0.0003864\n",
      "179\n",
      "input tokens: 2586; output tokens: 93, price: 0.0004437\n",
      "212\n",
      "input tokens: 2204; output tokens: 143, price: 0.00041639999999999993\n",
      "249\n",
      "input tokens: 1948; output tokens: 68, price: 0.000333\n",
      "272\n",
      "input tokens: 2306; output tokens: 92, price: 0.0004011\n",
      "313\n",
      "input tokens: 2023; output tokens: 114, price: 0.00037184999999999996\n",
      "358\n",
      "input tokens: 1555; output tokens: 89, price: 0.00028664999999999995\n",
      "389\n",
      "input tokens: 513; output tokens: 35, price: 9.795e-05\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "af5ce71cbc36b75b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3fc1aef3105e565"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a0ac81d2d3441e9b3a5df76a1bd2554": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_625a356a3c03490e94fd7fd4c6735dd5",
       "IPY_MODEL_9eae25c0de28460d8eda4c81f36b12c8",
       "IPY_MODEL_7aaeb859fd084de5a9e0a6a8c457530d"
      ],
      "layout": "IPY_MODEL_638b115d08bb4367a18d8cb83be3eaba"
     }
    },
    "625a356a3c03490e94fd7fd4c6735dd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b85add037e794592a3d10faafed14769",
      "placeholder": "​",
      "style": "IPY_MODEL_e6593af0c76145cb95e70163d8b7aa79",
      "value": "model.bin: 100%"
     }
    },
    "9eae25c0de28460d8eda4c81f36b12c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a037da54f3af40e6a8f2762d39c9223d",
      "max": 3087284237,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e29b8ba4a5034817a48d2467424f922b",
      "value": 3087284237
     }
    },
    "7aaeb859fd084de5a9e0a6a8c457530d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3990513292034aafae131b8045a203e9",
      "placeholder": "​",
      "style": "IPY_MODEL_c1da0bb480624ff0b39f7dd9fc5f9d0e",
      "value": " 3.09G/3.09G [00:13&lt;00:00, 262MB/s]"
     }
    },
    "638b115d08bb4367a18d8cb83be3eaba": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b85add037e794592a3d10faafed14769": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6593af0c76145cb95e70163d8b7aa79": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a037da54f3af40e6a8f2762d39c9223d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e29b8ba4a5034817a48d2467424f922b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3990513292034aafae131b8045a203e9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1da0bb480624ff0b39f7dd9fc5f9d0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "575173787b5c4eb1b445d2dec7bae387": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1ec2b1f01be44544bf5a4cb59f4e7499",
       "IPY_MODEL_17a44e75a6414564b01a9f053ebe8687",
       "IPY_MODEL_11b80bf54a9e42bc99d936b63b464dba"
      ],
      "layout": "IPY_MODEL_95a2cbd3dfe346d4a4d5337389a644f2"
     }
    },
    "1ec2b1f01be44544bf5a4cb59f4e7499": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fe82db2540845f985804af8c3712c40",
      "placeholder": "​",
      "style": "IPY_MODEL_12cc093257a5494fa42b75977e79d62c",
      "value": "config.json: 100%"
     }
    },
    "17a44e75a6414564b01a9f053ebe8687": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_328692e5bc6d40178ecc999b374f7632",
      "max": 2394,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_593ff4c702d3449fb4f548bdddbb6977",
      "value": 2394
     }
    },
    "11b80bf54a9e42bc99d936b63b464dba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_974273a5cbb64630b28f49f4622181f1",
      "placeholder": "​",
      "style": "IPY_MODEL_f19d9c65f1cf4af893d71fb2189beb5d",
      "value": " 2.39k/2.39k [00:00&lt;00:00, 47.2kB/s]"
     }
    },
    "95a2cbd3dfe346d4a4d5337389a644f2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fe82db2540845f985804af8c3712c40": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12cc093257a5494fa42b75977e79d62c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "328692e5bc6d40178ecc999b374f7632": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "593ff4c702d3449fb4f548bdddbb6977": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "974273a5cbb64630b28f49f4622181f1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f19d9c65f1cf4af893d71fb2189beb5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9346d22a11e4a5dbfc8e265d7603c4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bdb99e3a45464eb4a52d00538f5db3b9",
       "IPY_MODEL_7391cc19998e426288d31a3ad58b67b1",
       "IPY_MODEL_7b8979ebd440484aa62c64a70a95b585"
      ],
      "layout": "IPY_MODEL_ef51ff208ad54d5880fe097273503134"
     }
    },
    "bdb99e3a45464eb4a52d00538f5db3b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87fa8610eb184d06a5fa2525e0221fbf",
      "placeholder": "​",
      "style": "IPY_MODEL_7856dd4417c14c76afd1b8fb3aa2d151",
      "value": "tokenizer.json: 100%"
     }
    },
    "7391cc19998e426288d31a3ad58b67b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6c93bc03f6d4573913b489a73694c54",
      "max": 2480617,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_362358622d4047e8b1fb24a958315a02",
      "value": 2480617
     }
    },
    "7b8979ebd440484aa62c64a70a95b585": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25747c9a3e3e47bc8d1067d65026fa22",
      "placeholder": "​",
      "style": "IPY_MODEL_9e99c0ed5ccb4de9bc67c77d2e805a8a",
      "value": " 2.48M/2.48M [00:00&lt;00:00, 5.01MB/s]"
     }
    },
    "ef51ff208ad54d5880fe097273503134": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87fa8610eb184d06a5fa2525e0221fbf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7856dd4417c14c76afd1b8fb3aa2d151": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f6c93bc03f6d4573913b489a73694c54": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "362358622d4047e8b1fb24a958315a02": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25747c9a3e3e47bc8d1067d65026fa22": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e99c0ed5ccb4de9bc67c77d2e805a8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de6bb81359e74ab3b26d3ab321d7c789": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20b2418166fa4d0caa928ac2d390ad78",
       "IPY_MODEL_2b5f0fd2b89c48ebb1b5487691f35411",
       "IPY_MODEL_feca3fad25ac4c4d8c71a3189f0ef1f7"
      ],
      "layout": "IPY_MODEL_69efc6afcfd444ae9ecb9ca5e70e0d5f"
     }
    },
    "20b2418166fa4d0caa928ac2d390ad78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2827314ccbdc49079e3fea36c335b588",
      "placeholder": "​",
      "style": "IPY_MODEL_8329efacee6b4d0bb321b11d56a357c7",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "2b5f0fd2b89c48ebb1b5487691f35411": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92b5ded7cbf4467086af505723ad4e6d",
      "max": 340,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a19f4a06f3374e568acb51007656cea2",
      "value": 340
     }
    },
    "feca3fad25ac4c4d8c71a3189f0ef1f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ad99df8f8b244b3861a49cd66a8de9c",
      "placeholder": "​",
      "style": "IPY_MODEL_160bcbf3fc7e47aa83ac839dcef9e542",
      "value": " 340/340 [00:00&lt;00:00, 10.7kB/s]"
     }
    },
    "69efc6afcfd444ae9ecb9ca5e70e0d5f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2827314ccbdc49079e3fea36c335b588": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8329efacee6b4d0bb321b11d56a357c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "92b5ded7cbf4467086af505723ad4e6d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a19f4a06f3374e568acb51007656cea2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6ad99df8f8b244b3861a49cd66a8de9c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "160bcbf3fc7e47aa83ac839dcef9e542": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24a4c950d74a41f29bf8b074dbc2997f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ed5fa7711fac4e08b9f3882a269dd612",
       "IPY_MODEL_ca7e477fbecd4d00b29db417663d5d54",
       "IPY_MODEL_4bc292ec121c432f8ce0d71b70c37e30"
      ],
      "layout": "IPY_MODEL_7d596ba8913842a1ba53a37d3b4d6728"
     }
    },
    "ed5fa7711fac4e08b9f3882a269dd612": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5dbed46f7844ea6851b42af3766470f",
      "placeholder": "​",
      "style": "IPY_MODEL_ce34ab0c3707422090e4f801fd927805",
      "value": "vocabulary.json: 100%"
     }
    },
    "ca7e477fbecd4d00b29db417663d5d54": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5e7b727f99e4df4a2cc890771230d2f",
      "max": 1068114,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bb4794ca6cf742b3880fd16cb26ad28d",
      "value": 1068114
     }
    },
    "4bc292ec121c432f8ce0d71b70c37e30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_090f3982133a4bb38a41cae71300389a",
      "placeholder": "​",
      "style": "IPY_MODEL_a552975238a944d8a50d1ddf9400c287",
      "value": " 1.07M/1.07M [00:00&lt;00:00, 6.48MB/s]"
     }
    },
    "7d596ba8913842a1ba53a37d3b4d6728": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5dbed46f7844ea6851b42af3766470f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce34ab0c3707422090e4f801fd927805": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5e7b727f99e4df4a2cc890771230d2f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb4794ca6cf742b3880fd16cb26ad28d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "090f3982133a4bb38a41cae71300389a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a552975238a944d8a50d1ddf9400c287": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
